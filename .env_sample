# ============================================================================
# Audiobook Creator - Environment Configuration
# ============================================================================
# This configuration works on Mac, Linux, and Windows.
# All models are downloaded automatically on first run.
# ============================================================================

# ============================================================================
# LLM Configuration (for character identification & emotion tags)
# ============================================================================
# Using llama.cpp with GPT-OSS-20B (high quality, requires good hardware)
LLM_BASE_URL=http://llm_server:8000/v1
LLM_API_KEY=not-needed
LLM_MODEL_NAME=local-llm

# LLM Model Selection (GGUF format)
# Default: GPT-OSS-20B Q4 (~12GB download, best quality)
LLM_MODEL_FILE=gpt-oss-20b-Q4_K_M.gguf
LLM_HF_REPO=unsloth/gpt-oss-20b-GGUF

# Alternative: Smaller model for limited hardware (uncomment to use)
# LLM_MODEL_FILE=Llama-3.2-3B-Instruct-Q4_K_M.gguf
# LLM_HF_REPO=bartowski/Llama-3.2-3B-Instruct-GGUF

# LLM performance settings
LLM_CTX_SIZE=16384
LLM_THREADS=8
LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE=1

# Disable thinking mode (faster inference)
NO_THINK_MODE=true

# ============================================================================
# TTS Configuration (Orpheus TTS)
# ============================================================================
# Available voices: tara, leah, jess, leo, dan, mia, zac, zoe
TTS_BASE_URL=http://orpheus_tts:8880/v1
TTS_API_KEY=not-needed

# Parallel TTS requests (keep at 1 for CPU)
TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE=1

# ============================================================================
# Orpheus TTS Model Configuration
# ============================================================================
# Model options (GGUF format from unsloth on HuggingFace):
#   orpheus-3b-0.1-ft-Q4_K_M.gguf (default, ~2.1GB, good balance)
#   orpheus-3b-0.1-ft-Q8_0.gguf (~3.5GB, highest quality)
#   orpheus-3b-0.1-ft-Q2_K.gguf (~1.4GB, fastest)
ORPHEUS_MODEL_NAME=orpheus-3b-0.1-ft-Q8_0.gguf
ORPHEUS_HF_REPO=unsloth/orpheus-3b-0.1-ft-GGUF

# Orpheus performance settings
ORPHEUS_CTX_SIZE=4096
ORPHEUS_MAX_TOKENS=4096
ORPHEUS_THREADS=4

# Orpheus generation parameters
ORPHEUS_API_TIMEOUT=300
ORPHEUS_TEMPERATURE=0.6
ORPHEUS_TOP_P=0.9
ORPHEUS_SAMPLE_RATE=24000

# ============================================================================
# Compute Device Configuration
# ============================================================================
# GPU_LAYERS: Number of layers to offload to GPU
#   - Set to 0 for CPU-only mode
#   - Set to 99 or higher for full GPU offload (recommended if you have GPU)
#   - On Mac without NVIDIA GPU, keep at 0 (llama.cpp image requires NVIDIA)
#   - On Linux/Windows with NVIDIA GPU, set to 99 for best performance
GPU_LAYERS=95

# Compute device for Orpheus TTS SNAC model (auto/cuda/mps/cpu)
# This is for PyTorch in the orpheus_tts container
COMPUTE_DEVICE=auto

# Force CPU mode for PyTorch (set to true to disable GPU even if available)
FORCE_CPU=false
