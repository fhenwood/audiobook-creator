services:
  # Init container to download models - uses alpine with curl for proper permissions
  model_downloader:
    image: alpine:latest
    volumes:
      - orpheus_gguf_models:/models/orpheus
      - llm_models:/models/llm
    entrypoint: /bin/sh
    command:
      - -c
      - |
        set -e
        apk add --no-cache curl

        download_model() {
          local url="$$1"
          local output="$$2"
          local name="$$3"
          
          echo "Downloading $$name..."
          # Download with progress, follow redirects, fail on HTTP errors
          if curl -fL --progress-bar -o "$$output.tmp" "$$url"; then
            # Check if it's a valid GGUF file (magic bytes: GGUF)
            if head -c 4 "$$output.tmp" | grep -q "GGUF"; then
              mv "$$output.tmp" "$$output"
              echo "✓ $$name downloaded successfully"
              return 0
            else
              echo "✗ Downloaded file is not a valid GGUF model"
              rm -f "$$output.tmp"
              return 1
            fi
          else
            echo "✗ Failed to download $$name"
            rm -f "$$output.tmp"
            return 1
          fi
        }

        echo "=== Checking for Orpheus TTS model ==="
        if [ -f "/models/orpheus/${ORPHEUS_MODEL_NAME}" ]; then
          if head -c 4 "/models/orpheus/${ORPHEUS_MODEL_NAME}" | grep -q "GGUF"; then
            echo "✓ Orpheus model already exists and is valid"
          else
            echo "Existing Orpheus model is invalid, re-downloading..."
            rm -f "/models/orpheus/${ORPHEUS_MODEL_NAME}"
            download_model "https://huggingface.co/${ORPHEUS_HF_REPO}/resolve/main/${ORPHEUS_MODEL_NAME}" \
              "/models/orpheus/${ORPHEUS_MODEL_NAME}" "${ORPHEUS_MODEL_NAME}"
          fi
        else
          download_model "https://huggingface.co/${ORPHEUS_HF_REPO}/resolve/main/${ORPHEUS_MODEL_NAME}" \
            "/models/orpheus/${ORPHEUS_MODEL_NAME}" "${ORPHEUS_MODEL_NAME}"
        fi

        echo ""
        echo "=== Checking for LLM model ==="
        if [ -f "/models/llm/${LLM_MODEL_FILE}" ]; then
          if head -c 4 "/models/llm/${LLM_MODEL_FILE}" | grep -q "GGUF"; then
            echo "✓ LLM model already exists and is valid"
          else
            echo "Existing LLM model is invalid, re-downloading..."
            rm -f "/models/llm/${LLM_MODEL_FILE}"
            download_model "https://huggingface.co/${LLM_HF_REPO}/resolve/main/${LLM_MODEL_FILE}" \
              "/models/llm/${LLM_MODEL_FILE}" "${LLM_MODEL_FILE}"
          fi
        else
          download_model "https://huggingface.co/${LLM_HF_REPO}/resolve/main/${LLM_MODEL_FILE}" \
            "/models/llm/${LLM_MODEL_FILE}" "${LLM_MODEL_FILE}"
        fi

        echo ""
        echo "=== Model download check complete! ==="
    environment:
      - ORPHEUS_MODEL_NAME=${ORPHEUS_MODEL_NAME}
      - ORPHEUS_HF_REPO=${ORPHEUS_HF_REPO}
      - LLM_MODEL_FILE=${LLM_MODEL_FILE}
      - LLM_HF_REPO=${LLM_HF_REPO}

  # Main audiobook creator app (includes Gradio UI + Orpheus TTS API)
  audiobook_creator:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "7860:7860" # Gradio Web UI
      - "8880:8880" # Orpheus TTS API
    volumes:
      - .:/app
      - orpheus_snac_models:/root/.cache/huggingface
      - chatterbox_models:/root/.cache/chatterbox
      # Persist temp audio files for job resume capability
      - audiobook_temp_audio:/app/temp_audio
      # Mount docker socket for GPU resource management (start/stop LLM containers on-demand)
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - TTS_BASE_URL=http://localhost:8880/v1
      - ORPHEUS_API_URL=http://orpheus_llama:5006/v1/completions
      - LLM_BASE_URL=http://llm_server:8000/v1
      - GRADIO_SERVER_NAME=0.0.0.0
      - FORCE_CPU=${FORCE_CPU:-false}
      # GPU Resource Management settings
      - GPU_IDLE_TIMEOUT=${GPU_IDLE_TIMEOUT:-60}
      - LLM_SERVER_HOST=llm_server
      - LLM_SERVER_PORT=8000
      - ORPHEUS_LLAMA_HOST=orpheus_llama
      - ORPHEUS_LLAMA_PORT=5006
      # Performance tuning - parallel processing
      - TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE=${TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE:-8}
      - LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE=${LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE:-4}
      # VibeVoice TTS settings
      - VIBEVOICE_API_URL=http://vibevoice:8882
    depends_on:
      model_downloader:
        condition: service_completed_successfully
      orpheus_llama:
        condition: service_started
      llm_server:
        condition: service_started
    restart: unless-stopped

  # llama.cpp server for Orpheus TTS token generation
  orpheus_llama:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "5006:5006"
    volumes:
      - orpheus_gguf_models:/models:ro
    command: >
      --model /models/${ORPHEUS_MODEL_NAME} --ctx-size 8192 --host 0.0.0.0 --port 5006 --n-gpu-layers ${GPU_LAYERS:-0} --parallel ${TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE:-8} --cont-batching --flash-attn on
    depends_on:
      model_downloader:
        condition: service_completed_successfully
    restart: unless-stopped

  # llama.cpp server for LLM (character extraction, emotion tags)
  llm_server:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8000:8000"
    volumes:
      - llm_models:/models:ro
    command: >
      --model /models/${LLM_MODEL_FILE} --ctx-size 16384 --host 0.0.0.0 --port 8000 --n-gpu-layers ${GPU_LAYERS:-0} --parallel ${LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE:-4} --cont-batching --flash-attn on
    depends_on:
      model_downloader:
        condition: service_completed_successfully
    restart: unless-stopped

volumes:
  orpheus_snac_models:
  orpheus_gguf_models:
  llm_models:
  chatterbox_models:
  audiobook_output:
  audiobook_temp_audio:
  vibevoice_models:
  vibevoice_cache:
